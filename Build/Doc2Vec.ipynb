{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the doc2vec algorithm from Le and Mikolov (2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an stemmer object\n",
    "ps= PorterStemmer() \n",
    "# list of stop-words \n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "### create punctuation objects\n",
    "mypunct = \"\"\"!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~“”’…'\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add more stopwords so that i'm and im (or you'd and youd) would be in the list\n",
    "stop_list = []\n",
    "for stop in STOPWORDS:\n",
    "    if \"'\" in stop:\n",
    "        stop_list.append(re.sub(\"'\", \"\", stop))\n",
    "### update\n",
    "STOPWORDS.update(stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import 62 text files (60 ads and 2 aggregated public speech data)\n",
    "### 60 ad creatives were downloaded from Kantar Media Stradegy\n",
    "### 2 aggregated publich speech data (during the primaries) are downloaded from American Presidency Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "### locations where text files are saved\n",
    "folder = 'ALL_ADS_AND_SPEECHES_FINAL'\n",
    "file_loc = 'C:\\\\Users\\\\donggwan.kim\\\\Desktop\\\\Video_Transcribing_Final_corrected\\\\' + folder + '\\\\*.txt'\n",
    "file_paths = glob.glob(file_loc)\n",
    "print(len(file_paths))\n",
    "\n",
    "### \n",
    "name_list = []\n",
    "string_list = []\n",
    "for file in file_paths:\n",
    "    # get the filename\n",
    "    file_name = [name[0:8] for name in file.split('\\\\') if name[0].isdigit()][0]\n",
    "    # append it to the name_list\n",
    "    name_list.append(file_name)    \n",
    "    # open the text file\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    # concat each line and create one single string\n",
    "    string_concat = ''\n",
    "    for line in lines:\n",
    "        string_concat = string_concat + ' ' + line\n",
    "    # clean the string\n",
    "    string_concat = string_concat[1:]\n",
    "    string_concat = string_concat.replace(\"—\", \" \")\n",
    "    string_concat = string_concat.replace(\"-\", \" \")\n",
    "    # initial pre-processing\n",
    "    outcome = string_concat.lower() # lower case\n",
    "    outcome = outcome.strip() # remove some weird whitespaces\n",
    "    outcome = outcome.replace(\"\\n\", \"\") # remove \\n new lines\n",
    "    outcome = re.sub(r'\\d+', '', outcome) # remove numbers\n",
    "    # remove punctiations\n",
    "    outcome = re.sub('['+mypunct+']', '', outcome)\n",
    "    # remove extra spaces in strings\n",
    "    outcome = re.sub(r' +', ' ', outcome)\n",
    "    # tokenization\n",
    "    Tokenized = [word for word in outcome.split(' ') if (len(word) >= 2)]\n",
    "    # Remove numbers\n",
    "    Tokenized = [word for word in Tokenized if not word.isdigit()]\n",
    "    # stem\n",
    "    Token_Stem = [ps.stem(word) for word in Tokenized]\n",
    "    # final outcome\n",
    "    result = ' '.join(Token_Stem)\n",
    "    # append to string_list\n",
    "    string_list.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in zip(name_list, string_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 200\n",
    "alpha = 0.0025\n",
    "model = Doc2Vec(vector_size = vec_size, \n",
    "                min_count = 3, \n",
    "                window = 5, \n",
    "                negative = 5,\n",
    "                sample = 1e-2, \n",
    "                epochs = 300)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [str(num) for num in range(vec_size)]\n",
    "# create an empty table\n",
    "df_vector = pd.DataFrame(columns = col_names, index = np.arange(len(name_list)))\n",
    "# fill the table\n",
    "name_list_2 = [int(name) for name in name_list]\n",
    "for i in range(len(name_list)):\n",
    "    #vec = model.docvecs[name_list[i]]\n",
    "    df_vector.iloc[int(i)] = model.docvecs[name_list[i]]\n",
    "df_vector['FILE_NUM_RA_CODING'] = name_list_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = 'C:\\\\Users\\\\donggwan.kim\\\\Desktop\\\\doc2vec_w_200.csv'\n",
    "df_vector.to_csv(csv_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

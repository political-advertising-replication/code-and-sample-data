{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the doc2vec algorithm from Le and Mikolov (2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an stemmer object\n",
    "ps= PorterStemmer() \n",
    "# list of stop-words \n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "### create punctuation objects\n",
    "mypunct = \"\"\"!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~“”’…'\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add more stopwords so that i'm and im (or you'd and youd) would be in the list\n",
    "stop_list = []\n",
    "for stop in STOPWORDS:\n",
    "    if \"'\" in stop:\n",
    "        stop_list.append(re.sub(\"'\", \"\", stop))\n",
    "### update\n",
    "STOPWORDS.update(stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import 62 text files (60 ads and 2 aggregated public speech data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "### locations where text files are saved\n",
    "folder = 'ALL_ADS_AND_SPEECHES_FINAL'\n",
    "file_loc = 'C:\\\\Users\\\\donggwan.kim\\\\Desktop\\\\Video_Transcribing_Final_corrected\\\\' + folder + '\\\\*.txt'\n",
    "file_paths = glob.glob(file_loc)\n",
    "print(len(file_paths))\n",
    "\n",
    "### \n",
    "name_list = []\n",
    "string_list = []\n",
    "for file in file_paths:\n",
    "    # get the filename\n",
    "    file_name = [name[0:8] for name in file.split('\\\\') if name[0].isdigit()][0]\n",
    "    # append it to the name_list\n",
    "    name_list.append(file_name)    \n",
    "    # open the text file\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    # concat each line and create one single string\n",
    "    string_concat = ''\n",
    "    for line in lines:\n",
    "        string_concat = string_concat + ' ' + line\n",
    "    # clean the string\n",
    "    string_concat = string_concat[1:]\n",
    "    string_concat = string_concat.replace(\"—\", \" \")\n",
    "    string_concat = string_concat.replace(\"-\", \" \")\n",
    "    # initial pre-processing\n",
    "    outcome = string_concat.lower() # lower case\n",
    "    outcome = outcome.strip() # remove some weird whitespaces\n",
    "    outcome = outcome.replace(\"\\n\", \"\") # remove \\n new lines\n",
    "    outcome = re.sub(r'\\d+', '', outcome) # remove numbers\n",
    "    # remove punctiations\n",
    "    outcome = re.sub('['+mypunct+']', '', outcome)\n",
    "    # remove extra spaces in strings\n",
    "    outcome = re.sub(r' +', ' ', outcome)\n",
    "    # tokenization\n",
    "    Tokenized = [word for word in outcome.split(' ') if (len(word) >= 2)]\n",
    "    # Remove numbers\n",
    "    Tokenized = [word for word in Tokenized if not word.isdigit()]\n",
    "    # stem\n",
    "    Token_Stem = [ps.stem(word) for word in Tokenized]\n",
    "    # final outcome\n",
    "    result = ' '.join(Token_Stem)\n",
    "    # append to string_list\n",
    "    string_list.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in zip(name_list, string_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "vec_size = 200\n",
    "alpha = 0.0025\n",
    "model = Doc2Vec(vector_size = vec_size, \n",
    "                min_count = 3, \n",
    "                window = 5, \n",
    "                negative = 5,\n",
    "                sample = 1e-2, \n",
    "                epochs = 300)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>FILE_NUM_RA_CODING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.226</td>\n",
       "      <td>1.23265</td>\n",
       "      <td>3.26522</td>\n",
       "      <td>2.01937</td>\n",
       "      <td>-4.30425</td>\n",
       "      <td>1.06146</td>\n",
       "      <td>0.189681</td>\n",
       "      <td>0.856751</td>\n",
       "      <td>1.16228</td>\n",
       "      <td>0.311134</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.93005</td>\n",
       "      <td>-1.49484</td>\n",
       "      <td>-0.861104</td>\n",
       "      <td>0.925943</td>\n",
       "      <td>1.92489</td>\n",
       "      <td>-0.827523</td>\n",
       "      <td>0.406514</td>\n",
       "      <td>0.679727</td>\n",
       "      <td>1.85692</td>\n",
       "      <td>16667355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.981825</td>\n",
       "      <td>-0.982362</td>\n",
       "      <td>0.907243</td>\n",
       "      <td>1.0505</td>\n",
       "      <td>0.829617</td>\n",
       "      <td>-0.419295</td>\n",
       "      <td>0.595089</td>\n",
       "      <td>-2.21098</td>\n",
       "      <td>2.00307</td>\n",
       "      <td>1.31757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311418</td>\n",
       "      <td>0.480255</td>\n",
       "      <td>1.0363</td>\n",
       "      <td>0.551928</td>\n",
       "      <td>-1.67862</td>\n",
       "      <td>-0.310181</td>\n",
       "      <td>-1.6797</td>\n",
       "      <td>0.740321</td>\n",
       "      <td>-1.41946</td>\n",
       "      <td>16667854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.144669</td>\n",
       "      <td>0.931877</td>\n",
       "      <td>1.98057</td>\n",
       "      <td>0.0250422</td>\n",
       "      <td>-2.21809</td>\n",
       "      <td>-0.269239</td>\n",
       "      <td>-0.767738</td>\n",
       "      <td>1.0454</td>\n",
       "      <td>2.21797</td>\n",
       "      <td>0.423852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.13085</td>\n",
       "      <td>-0.188902</td>\n",
       "      <td>-1.21657</td>\n",
       "      <td>1.89988</td>\n",
       "      <td>-1.2813</td>\n",
       "      <td>0.168974</td>\n",
       "      <td>1.56463</td>\n",
       "      <td>1.59248</td>\n",
       "      <td>-2.36729</td>\n",
       "      <td>16668348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.31907</td>\n",
       "      <td>0.800541</td>\n",
       "      <td>0.0715181</td>\n",
       "      <td>3.1806</td>\n",
       "      <td>0.723967</td>\n",
       "      <td>-0.289595</td>\n",
       "      <td>-1.2937</td>\n",
       "      <td>-0.825697</td>\n",
       "      <td>1.81298</td>\n",
       "      <td>-0.151762</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.64074</td>\n",
       "      <td>1.39492</td>\n",
       "      <td>-0.11056</td>\n",
       "      <td>2.09266</td>\n",
       "      <td>0.85668</td>\n",
       "      <td>0.557391</td>\n",
       "      <td>-0.226844</td>\n",
       "      <td>0.888729</td>\n",
       "      <td>-2.0393</td>\n",
       "      <td>16695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.80005</td>\n",
       "      <td>0.0364885</td>\n",
       "      <td>0.411981</td>\n",
       "      <td>1.77444</td>\n",
       "      <td>-2.06352</td>\n",
       "      <td>-0.798525</td>\n",
       "      <td>2.1172</td>\n",
       "      <td>-2.13095</td>\n",
       "      <td>3.06475</td>\n",
       "      <td>2.18534</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.06533</td>\n",
       "      <td>0.581341</td>\n",
       "      <td>2.15495</td>\n",
       "      <td>-0.884233</td>\n",
       "      <td>1.38538</td>\n",
       "      <td>-1.15336</td>\n",
       "      <td>-1.04229</td>\n",
       "      <td>0.461974</td>\n",
       "      <td>0.454172</td>\n",
       "      <td>16707654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1          2          3         4         5         6  \\\n",
       "0     2.226    1.23265    3.26522    2.01937  -4.30425   1.06146  0.189681   \n",
       "1  0.981825  -0.982362   0.907243     1.0505  0.829617 -0.419295  0.595089   \n",
       "2  0.144669   0.931877    1.98057  0.0250422  -2.21809 -0.269239 -0.767738   \n",
       "3   2.31907   0.800541  0.0715181     3.1806  0.723967 -0.289595   -1.2937   \n",
       "4  -0.80005  0.0364885   0.411981    1.77444  -2.06352 -0.798525    2.1172   \n",
       "\n",
       "          7        8         9  ...       191       192       193       194  \\\n",
       "0  0.856751  1.16228  0.311134  ...  -1.93005  -1.49484 -0.861104  0.925943   \n",
       "1  -2.21098  2.00307   1.31757  ...  0.311418  0.480255    1.0363  0.551928   \n",
       "2    1.0454  2.21797  0.423852  ...  -0.13085 -0.188902  -1.21657   1.89988   \n",
       "3 -0.825697  1.81298 -0.151762  ...  -1.64074   1.39492  -0.11056   2.09266   \n",
       "4  -2.13095  3.06475   2.18534  ...  -1.06533  0.581341   2.15495 -0.884233   \n",
       "\n",
       "       195       196       197       198       199 FILE_NUM_RA_CODING  \n",
       "0  1.92489 -0.827523  0.406514  0.679727   1.85692           16667355  \n",
       "1 -1.67862 -0.310181   -1.6797  0.740321  -1.41946           16667854  \n",
       "2  -1.2813  0.168974   1.56463   1.59248  -2.36729           16668348  \n",
       "3  0.85668  0.557391 -0.226844  0.888729   -2.0393           16695900  \n",
       "4  1.38538  -1.15336  -1.04229  0.461974  0.454172           16707654  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = [str(num) for num in range(vec_size)]\n",
    "# create an empty table\n",
    "df_vector = pd.DataFrame(columns = col_names, index = np.arange(len(name_list)))\n",
    "# fill the table\n",
    "name_list_2 = [int(name) for name in name_list]\n",
    "for i in range(len(name_list)):\n",
    "    #vec = model.docvecs[name_list[i]]\n",
    "    df_vector.iloc[int(i)] = model.docvecs[name_list[i]]\n",
    "df_vector['FILE_NUM_RA_CODING'] = name_list_2\n",
    "df_vector.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donggwan.kim\\Box\\Documents\\Dropbox_RT_New\\doc2vec_w_200.csv\n"
     ]
    }
   ],
   "source": [
    "csv_name = 'C:\\\\Users\\\\donggwan.kim\\\\Box\\\\Documents\\\\Dropbox_RT_New\\\\doc2vec_w_200.csv'\n",
    "df_vector.to_csv(csv_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

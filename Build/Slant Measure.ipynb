{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the slant measure from Gentzkow and Shapiro (2010) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\donggwan.kim\\\\Desktop\\hein-daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker - Speech - Chamber - Party data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data are downloaded from https://data.stanford.edu/congress_text\n",
    "speaker_df = pd.read_csv('speaker_df.csv')\n",
    "speaker_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# add additional stopwords (Fox's stopwords) that Gentzknow and Shapiro used in their paper.\n",
    "# source: https://gist.github.com/maxwelld90/6bafbf2570877c4d1de0\n",
    "with open('classical_stopwords.txt') as f:\n",
    "    lines = f.readlines()\n",
    "FOX_stopwords = set([word.rstrip() for word in lines[5:]]) # remove white space \n",
    "\n",
    "# merge the two lists\n",
    "STOPWORDS.update(FOX_stopwords)\n",
    "\n",
    "# create a stemmer object\n",
    "ps= PorterStemmer()\n",
    "\n",
    "# punctuation\n",
    "string.punctuation = string.punctuation + '•'\n",
    "\n",
    "# add more stopwords to improve precision\n",
    "# for higher precision, have both aren't and arent  or i'm and im\n",
    "STOPWORDS_wo_punc = []\n",
    "for word in list(STOPWORDS):\n",
    "    word = re.sub('['+string.punctuation+']', '', word)\n",
    "    STOPWORDS_wo_punc.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) For each word, get the total number of times it was used by each party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_gram_function (arg):\n",
    "    # make it lowercases\n",
    "    arg = arg.lower()\n",
    "    # remove punctiations\n",
    "    import string\n",
    "    arg = re.sub('['+string.punctuation+']', '', arg)\n",
    "    # remove extra spaces in strings\n",
    "    arg = re.sub(r' +', ' ', arg)\n",
    "    # string replacement - we found that these words are used with and without a space\n",
    "    # as this is an important topic in ads, we replace the space for consistency\n",
    "    arg = arg.replace('health care', 'healthcare')\n",
    "    arg = arg.replace('child care', 'childcare')\n",
    "    # tokenize - remove if a word is too short or a stopword\n",
    "    Tokenized = [word for word in word_tokenize(arg) if ((len(word) >= 2) & (word not in STOPWORDS))]\n",
    "    # remove numbers\n",
    "    Tokenized = [word for word in Tokenized if not word.isdigit()]\n",
    "    # stem - double check if there are remaining stopwords\n",
    "    Token_Stem = [ps.stem(word) for word in Tokenized if (word not in STOPWORDS)]\n",
    "    # counter\n",
    "    Freq_1 = Counter(list(Token_Stem))\n",
    "    # return the counter\n",
    "    return Freq_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Republicans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rep = ''\n",
    "for text in speaker_df[speaker_df['party'] == 'R']['text']:\n",
    "    Rep = Rep + ' ' + text\n",
    "Rep = Rep[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Democrats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dem = ''\n",
    "for text in speaker_df[speaker_df['party'] == 'D']['text']:\n",
    "    Dem = Dem + ' ' + text\n",
    "Dem = Dem[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply the counter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rep_count = one_gram_function(Rep)\n",
    "Dem_count = one_gram_function(Dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make them as dictionary objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rep_count_dict = dict(Rep_count)\n",
    "Dem_count_dict = dict(Dem_count)\n",
    "\n",
    "#  all words that we consider for chi-squared statistics\n",
    "all_words = list(Rep_count_dict.keys()) + list(Dem_count_dict.keys())\n",
    "all_words = list(set(all_words)) # about 100,000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Rep_count['childcar']) # 59\n",
    "print(Dem_count['childcar']) # 360\n",
    "print(Rep_count['terror']) # 2664\n",
    "print(Dem_count['terror']) # 1834"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) chi-squared statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate chi-squared stat for each word\n",
    "chi_squared_list = {}\n",
    "\n",
    "for word in all_words:\n",
    "    \n",
    "    # if found in Rep_count\n",
    "    if word in Rep_count.keys():\n",
    "        f_r = Rep_count_dict[word]\n",
    "    else:\n",
    "        f_r = 0\n",
    "        \n",
    "    # if found in Dem_count\n",
    "    if word in Dem_count.keys():\n",
    "        f_d = Dem_count_dict[word]\n",
    "    else: \n",
    "        f_d = 0\n",
    "        \n",
    "    f_not_r = sum(Rep_count.values()) - f_r\n",
    "    f_not_d = sum(Dem_count.values()) - f_d\n",
    "    \n",
    "    num = ((f_r * f_not_d - f_d * f_not_r) ** 2) \n",
    "    denom = (f_r + f_d) * (f_r + f_not_r) * (f_d + f_not_d) * (f_not_r + f_not_d)\n",
    "    chi_squared = num / denom\n",
    "    \n",
    "    chi_squared_list.update({word:float(chi_squared)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import (1) 60 ad creatives (transcribed texts) and (2) two aggregated speech data\n",
    "### ad creatives from Kantar Media cannot be shared due to NDA\n",
    "### speech data can be downloaded from American Presidency Project (https://www.presidency.ucsb.edu/)\n",
    "### we collect all public speech data from both candidates during the primary election\n",
    "folder = 'ALL_ADS_AND_SPEECHES_FINAL'\n",
    "file_loc = 'C:\\\\Users\\\\donggwan.kim\\\\Desktop\\\\Video_Transcribing_Final_corrected\\\\ALL_ADS_AND_SPEECHES_FINAL\\\\*.txt'\n",
    "file_paths = glob.glob(file_loc)\n",
    "print(len(file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = []\n",
    "string_list = []\n",
    "\n",
    "for file in file_paths:\n",
    "    ### get fila names only\n",
    "    file_name = [name[0:8] for name in file.split('\\\\') if name[0].isdigit()][0]\n",
    "    ### append it to the name list\n",
    "    name_list.append(file_name)\n",
    "    ### open the text file\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    ### create a string\n",
    "    string_concat = ''\n",
    "    for line in lines:\n",
    "        string_concat = string_concat + ' ' + line\n",
    "    ### replace some characters\n",
    "    string_concat = string_concat.replace(\"—\", \" \")\n",
    "    string_concat = string_concat.replace(\"–\", \" \")\n",
    "    string_concat = string_concat[1:]\n",
    "    ### initial pre-processing\n",
    "    outcome = string_concat.lower() # lower case\n",
    "    outcome = outcome.strip() # remove some excessive whitespaces\n",
    "    outcome = outcome.replace(\"\\n\", \"\") # remove \"\\n\" new lines\n",
    "    outcome = re.sub(r'\\d+', '', outcome) # remove numbers\n",
    "    # remove punctuation\n",
    "    outcome = re.sub('['+string.punctuation+']', '', outcome)\n",
    "    string_list.append(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_and_speech = ''\n",
    "for string in string_list:\n",
    "    ad_and_speech = ad_and_speech + ' ' + string\n",
    "ad_and_speech = ad_and_speech[1:]\n",
    "\n",
    "Freq_1 = one_gram_function(ad_and_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = []\n",
    "\n",
    "for word in Freq_1.keys():\n",
    "    if (Freq_1[word] >= 2) & (Freq_1[word] <= 100):\n",
    "        feature_selection.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared_list_short = {}\n",
    "for word in feature_selection:\n",
    "    chi_squared_list_short[str(word)] = float(chi_squared_list[str(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get 1,000 words that are used by either of the two parties\n",
    "political_words = []\n",
    "NUM_OF_WORDS = 1000\n",
    "for word in feature_selection:\n",
    "    cutoff = sorted(chi_squared_list_short.values(), reverse=True)[int(NUM_OF_WORDS)]\n",
    "    if chi_squared_list_short[word] > cutoff:\n",
    "        political_words.append(word)\n",
    "print(len(political_words))\n",
    "\n",
    "### test\n",
    "print('nuclear' in political_words)\n",
    "print('childcar' in political_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) mapping phrases to ideology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative frequency by speaker among the selected words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_gram_function_2 (arg):\n",
    "    # make it lowercases\n",
    "    arg = arg.lower()\n",
    "    # remove punctiations\n",
    "    import string\n",
    "    arg = re.sub('['+string.punctuation+']', '', arg)\n",
    "    # remove extra spaces in strings\n",
    "    arg = re.sub(r' +', ' ', arg)\n",
    "    # tokenize\n",
    "    Tokenized = [word for word in word_tokenize(arg) if ((len(word) >= 2) & (word not in STOPWORDS))]\n",
    "    # Remove numbers\n",
    "    Tokenized = [word for word in Tokenized if not word.isdigit()]\n",
    "    # stem\n",
    "    Token_Stem = [ps.stem(word) for word in Tokenized if (word not in STOPWORDS)]\n",
    "    # the frequencies of words that are selected as political words\n",
    "    Token_Stem_2 = [word for word in Token_Stem if word in political_words]\n",
    "    # counter\n",
    "    Freq_1 = Counter(list(Token_Stem_2))\n",
    "    # return\n",
    "    return Freq_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_list = list(set(speaker_df['speaker_id']))\n",
    "a = 0\n",
    "new_list = []\n",
    "\n",
    "for speaker in speaker_list:    \n",
    "    # print number\n",
    "    print(a)\n",
    "    # for each speaker, I get their frequencies of the politcally charged words\n",
    "    temp_df = speaker_df[speaker_df['speaker_id'] == int(speaker)]\n",
    "    # get the frequency\n",
    "    word_freq = one_gram_function_2(temp_df.iloc[0]['text'])\n",
    "    # create an inner list - to get the frequency of each political word\n",
    "    inner_list = []\n",
    "    # political_words - list of the selected political words\n",
    "    for word in political_words: \n",
    "        if word in word_freq.keys():\n",
    "            inner_list.append(word_freq[str(word)])\n",
    "        else:\n",
    "            inner_list.append(0)\n",
    "    # get a vector for relative frequencies    \n",
    "    relative_freq = np.array(inner_list) / np.array(inner_list).sum()\n",
    "    # append the relative frequency vector to the empty list\n",
    "    new_list.append(relative_freq)\n",
    "    a += 1\n",
    "print(len((new_list[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty table\n",
    "regression_df = pd.DataFrame(columns = political_words, index = np.arange(len(speaker_list)))\n",
    "# fill it by row\n",
    "for i in range(len(new_list)):\n",
    "    regression_df.iloc[int(i)] = new_list[int(i)]    \n",
    "# add speaker information to the data frame\n",
    "regression_df['speaker_id'] = speaker_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add additional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = speaker_df[['speaker_id', 'lastname', 'firstname', 'chamber', 'state', 'party', 'district']].copy()\n",
    "tmp.rename(columns={'speaker_id': 'speaker_id', 'lastname': 'lastname',\n",
    "                    'firstname': 'firstname', 'chamber': 'chamber',\n",
    "                    'state': 'state_2', 'party': 'party', 'district': 'district'}, inplace=True)\n",
    "### merge\n",
    "combregression_df_2 = pd.merge(regression_df, tmp, on = 'speaker_id', how = 'left').copy()\n",
    "### fill NA b/c district is missing for S \n",
    "combregression_df_2[\"district\"].fillna(100, inplace = True)\n",
    "### add a constant \n",
    "combregression_df_2['constant'] = 1\n",
    "combregression_df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vote going to republican candidate  (collected from Daily Kos)\n",
    "### for senators use state level, for house representatives use district level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vote = pd.read_csv('senate_vote_share_going_to_rep.csv')\n",
    "df_vote_2 = pd.read_csv('house_vote_share_going_to_rep.csv')\n",
    "df_vote_2.drop(columns=['Unnamed: 0', 'fullname', 'party'], inplace = True)\n",
    "df_vote_2.rename(columns={'state': 'state_2', 'district': 'district',\n",
    "                          'chamber': 'chamber', 'rep_vote_share': 'rep_vote_share'}, inplace=True)\n",
    "df_vote_final = pd.concat([df_vote, df_vote_2], axis=0)\n",
    "df_vote_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge it to the main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combregression_df_3 = pd.merge(combregression_df_2, df_vote_final, \n",
    "                               on = ['chamber', 'state_2', 'district'], \n",
    "                               how = 'left').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.compat.pandas import Appender\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combregression_df_3.dropna(inplace = True)\n",
    "\n",
    "alpha_list = []\n",
    "beta_list = []\n",
    "length = len(political_words)\n",
    "print('total # of words:', length)\n",
    "\n",
    "for i in range(int(length)):\n",
    "    # select a word from the political word list\n",
    "    variable = political_words[int(i)]\n",
    "    # run regression\n",
    "    # y: f_{pc}\n",
    "    # x: a_{p}, b_{p}\n",
    "    sm_model = sm.OLS(combregression_df_3[str(variable)].astype(float), \n",
    "                      combregression_df_3[['constant', 'rep_vote_share']].astype(float)) # republican vs rep_vote_share\n",
    "    sm_fit = sm_model.fit()\n",
    "    alpha, beta = sm_fit.params    \n",
    "    alpha_list.append(alpha)\n",
    "    beta_list.append(beta)\n",
    "\n",
    "data_coef = pd.DataFrame(list(zip(political_words, alpha_list, beta_list)), columns = ['phrase', 'alpha', 'beta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) apply the mapping derived from 3) to ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'ALL_ADS_AND_SPEECHES_FINAL'\n",
    "file_loc = 'C:\\\\Users\\\\donggwan.kim\\\\Desktop\\\\Video_Transcribing_Final_corrected\\\\' + folder + '\\\\*.txt'\n",
    "file_paths = glob.glob(file_loc)\n",
    "file_paths = file_paths[0:60] ### the last two are speech data\n",
    "\n",
    "RA_FILE = []\n",
    "slant_index = []\n",
    "\n",
    "for file in file_paths:\n",
    "    ### open the transcribed test\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "    ### create a string\n",
    "    ad_text = ''  \n",
    "    for line in lines:\n",
    "        # nomination speech has some issues\n",
    "        line = line.replace(\"\\\\\", \"\")\n",
    "        ad_text = ad_text + ' ' + line.rstrip()\n",
    "    ad_text = ad_text[1:]\n",
    "    ### lower case\n",
    "    ad_text = ad_text.lower()\n",
    "    ### get frequency\n",
    "    Freq = one_gram_function_2(ad_text)\n",
    "    ### estimate the slant score\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    for word in political_words:\n",
    "        alpha = pd.to_numeric(data_coef[data_coef['phrase'] == word]['alpha'])\n",
    "        alpha = float(alpha)\n",
    "        beta = pd.to_numeric(data_coef[data_coef['phrase'] == word]['beta'])\n",
    "        beta = float(beta)\n",
    "        # if a given political word appears in the ad\n",
    "        if word in Freq.keys():\n",
    "            num = num + beta * ((Freq[word] / sum(Freq.values())) - alpha)\n",
    "            denom = denom + (beta * beta)\n",
    "        # otherwise - if the word does not appear in the ad\n",
    "        else:\n",
    "            num = num + beta * (0 - alpha)\n",
    "            denom = denom + (beta * beta)\n",
    "    y_hat = num / denom\n",
    "    ### save it\n",
    "    RA_FILE.append(file[92:100])        \n",
    "    slant_index.append(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame(list(zip([int(num) for num in RA_FILE], np.array(slant_index))), \n",
    "                       columns = ['FILE_NUM_RA_CODING', 'slant'])\n",
    "df_output.to_csv('slant_estimates.csv')\n",
    "df_output.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
